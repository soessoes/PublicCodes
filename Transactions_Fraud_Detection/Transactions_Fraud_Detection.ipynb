{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Soheil Esmaeilzadeh\n",
    "\n",
    "__Email:__ soheil.esmaeilzadeh@gmail.com / soes@stanford.edu\n",
    "\n",
    "__Date:__ 10/10/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions Fraud Detection\n",
    "Here we do __transaction fraud detection__ using:\n",
    "\n",
    "__1)__ Check locations of transaction - 2 different transactions in a short period. \n",
    "\n",
    "__2)__ User's Credit card use history \n",
    "\n",
    "__3)__ Classifcation on different frauds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "479748ca-639b-4448-ae21-3681170a65de",
    "_uuid": "22d41ba02b32da646889dba983ba08c08cb38f08"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "0841452d-e539-4ece-85d1-15d360043c09",
    "_uuid": "86182358544562cbe369b51e9814655acde61586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data set \n",
    "df = pd.read_csv(\"./input/creditcard.csv\")\n",
    "\n",
    "# check if there is any null values\n",
    "print((df.isnull().sum()).sum())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training dataset: Train, Dev, Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "029fac77-6ded-4a18-a970-e548a49692b4",
    "_kg_hide-input": false,
    "_uuid": "5906208cac3075b9b5d6889561210b3e990cd2ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "columns = \"Time V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount\".split()\n",
    "X = pd.DataFrame.as_matrix(df,columns=columns)\n",
    "Y = np.asanyarray(df.Class)\n",
    "Y = Y.reshape(Y.shape[0],1)\n",
    "X.shape\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.06)\n",
    "X_test, X_dev, Y_test, Y_dev = train_test_split(X_test,Y_test, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "99f15a8c-4d5f-4b5e-9af0-56ee0bdf2e87",
    "_uuid": "19e721fe5df41744cbf1669a5f6ca0dd817d7635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : (267718, 30)\n",
      "No of test Examples : 8544\n",
      "No of dev Examples : 8545\n",
      "Shape of training data : (267718, 30)\n",
      "Shape of test data : (8544, 30)\n",
      "Shape of dev data : (8545, 30)\n",
      "Shape of Y test data : (8544, 1)\n",
      "Shape of Y dev data : (8545, 1)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the data \n",
    "X_train_flatten = X_train.reshape(X_train.shape[0],-1).T\n",
    "Y_train_flatten = Y_train.reshape(Y_train.shape[0],-1).T\n",
    "X_dev_flatten = X_dev.reshape(X_dev.shape[0],-1).T\n",
    "Y_dev_flatten = Y_dev.reshape(Y_dev.shape[0],-1).T\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1).T\n",
    "Y_test_flatten = Y_test.reshape(Y_test.shape[0],-1).T\n",
    "\n",
    "# Checking the shape's of the new data set as matrix \n",
    "print(\"No of training Examples : \"+str(X_train.shape))  # 94% data \n",
    "print(\"No of test Examples : \"+str(X_test.shape[0]))       # 3% data\n",
    "print(\"No of dev Examples : \"+str(X_dev.shape[0]))         # 3% data\n",
    "print(\"Shape of training data : \"+str(X_train.shape))\n",
    "print(\"Shape of test data : \"+str(X_test.shape))\n",
    "print(\"Shape of dev data : \"+str(X_dev.shape))\n",
    "print(\"Shape of Y test data : \"+str(Y_test.shape))\n",
    "print(\"Shape of Y dev data : \"+str(Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "6a35d7a9-d1d7-4f24-b9a7-3e999579ec9c",
    "_uuid": "262fd43b1a97c87f2529dfcda8a6761089d92d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of X_train_set shape : (30, 267718)\n",
      "No of Y_train_set shape : (1, 267718)\n"
     ]
    }
   ],
   "source": [
    "# Normalize features and create final Training set \n",
    "X_train_set = preprocessing.normalize(X_train_flatten)\n",
    "Y_train_set = Y_train_flatten\n",
    "\n",
    "print(\"No of X_train_set shape : \"+str(X_train_set.shape))  \n",
    "print(\"No of Y_train_set shape : \"+str(Y_train_set.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "5f27ff04-874e-4d2f-8ca5-e7878fcd5e24",
    "_uuid": "f1aaea3cbf9087ee414188e394965e2eb2eb3a5d"
   },
   "outputs": [],
   "source": [
    "# Funcation to intialize weights for forward propogration \n",
    "def intialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l],1))\n",
    "            \n",
    "    return parameters\n",
    "\n",
    "# create the sigmoid function \n",
    "def sigmoid(z):\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    cache = z\n",
    "    return s,cache\n",
    "\n",
    "# Relu Backward and Sigmoid Backward\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "# create the relu function\n",
    "def relu(z):\n",
    "    \n",
    "    r = np.maximum(0,z)\n",
    "    cache = z\n",
    "    return r,cache\n",
    "# Relu Backward and Sigmoid Backward\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "# Linear_forward\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "#linear_activation_forward\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "# L layers forward propagation \n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A, cache = linear_activation_forward(A,parameters[\"W\" + str(l)],parameters[\"b\" + str(l)],activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A,parameters[\"W\" + str(L)],parameters[\"b\" + str(L)],activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "#  Cost function\n",
    "\n",
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# linear_backward \n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "# linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "# backward propagation\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "# update parameters \n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\"+str(l)]=parameters[\"W\" + str(l)]-learning_rate*grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\"+str(l)]=parameters[\"b\" + str(l)]-learning_rate*grads[\"db\" + str(l)]\n",
    "    return parameters\n",
    "\n",
    "# predict Function\n",
    "def predict(X, y, parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = forward_propagation(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)), '\\n')\n",
    "        \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "78646745-cd78-404f-940a-93e65f5a40c6",
    "_uuid": "3a728b579a2f41abe935e1509c87a4485ffc691a"
   },
   "outputs": [],
   "source": [
    "# setting the size of the network \n",
    "layer_dims = [30,20,10,5,1] #5 Layer model with 3 hidden layers \n",
    "\n",
    "# Deep Learning network to classify frauds and normal\n",
    "layer_dims = [30,20,10,5,1] #5 Layer model with 3 hidden layers \n",
    "\n",
    "# Deep Learning network to classify frauds and normal\n",
    "def nn_model(X,Y,layer_dims,learning_rate=.0065, num_iterations=2500,print_cost=False):\n",
    "    costs = []\n",
    "    \n",
    "    #initialize parameters \n",
    "    parameters = intialize_parameters(layer_dims)\n",
    "    # for loop for iterations/epoch \n",
    "    for i in range(0,num_iterations):\n",
    "        #forward_propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = cost_function(AL, Y)\n",
    "        \n",
    "        #backward_propagation \n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "be051add-e65c-43d4-a4a1-c03c6ca49ae1",
    "_uuid": "b2a5307516588e68850fe42fa8c61d305234bef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.555154\n",
      "Cost after iteration 200: 0.454717\n",
      "Cost after iteration 300: 0.380435\n",
      "Cost after iteration 400: 0.324373\n",
      "Cost after iteration 500: 0.281164\n",
      "Cost after iteration 600: 0.247183\n",
      "Cost after iteration 700: 0.219958\n",
      "Cost after iteration 800: 0.197780\n",
      "Cost after iteration 900: 0.179441\n",
      "Cost after iteration 1000: 0.164074\n",
      "Cost after iteration 1100: 0.151043\n",
      "Cost after iteration 1200: 0.139878\n",
      "Cost after iteration 1300: 0.130219\n",
      "Cost after iteration 1400: 0.121794\n",
      "Cost after iteration 1500: 0.114388\n",
      "Cost after iteration 1600: 0.107834\n",
      "Cost after iteration 1700: 0.101997\n",
      "Cost after iteration 1800: 0.096770\n",
      "Cost after iteration 1900: 0.092064\n",
      "Cost after iteration 2000: 0.087807\n",
      "Cost after iteration 2100: 0.083940\n",
      "Cost after iteration 2200: 0.080413\n",
      "Cost after iteration 2300: 0.077185\n",
      "Cost after iteration 2400: 0.074219\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# running the model and training\n",
    "parameters = nn_model(X_train_set,Y_train_set,layer_dims,learning_rate=.0065,num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "fb1de1e7-e8b5-42ed-b613-0fdf4a559da9",
    "_uuid": "9599ce43b5383c122e724b7105da3464d86306bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy is: \n",
      "Accuracy: 0.9983116562950571 \n",
      "\n",
      "dev set accuracy is: \n",
      "Accuracy: 0.9980105324751315 \n",
      "\n",
      "test set accuracy is: \n",
      "Accuracy: 0.9973080524344572 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train set accuracy is: ')\n",
    "\n",
    "pred_train = predict(X_train_set, Y_train_set, parameters)\n",
    "\n",
    "print('dev set accuracy is: ')\n",
    "pred_dev = predict(X_dev_flatten, Y_dev_flatten, parameters)\n",
    "\n",
    "print('test set accuracy is: ')\n",
    "pred_test = predict(X_test_flatten, Y_test_flatten, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
