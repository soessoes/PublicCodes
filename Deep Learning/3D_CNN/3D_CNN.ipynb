{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "#### We use tensorFlow to build the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportValues(namex,namey):\n",
    "   # This function loads the x and y values, reshapes X to volume and tranform Y to 1-hot \n",
    "    X=np.load(namex)\n",
    "    Y=np.load(namey)\n",
    "    Y=np.squeeze(Y)\n",
    "\n",
    "    # length check\n",
    "    assert(len(X) == len(Y))\n",
    "\n",
    "\n",
    "    n_classes = 3 # 3 labels: 0:normal, 1:MCI, 2:Alzh\n",
    "\n",
    "    X = np.reshape(X,(-1,256,256,170,1))\n",
    "\n",
    "    # choosing a 16 by 16 by 16 slot in the images\n",
    "    X =X[:, 100:116, 100:116, 100:116, :]\n",
    "\n",
    "    # change lables to 1-hot form matrix\n",
    "    Y = to_categorical(Y, n_classes)\n",
    "\n",
    "    return X ,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiton of Convolutional Newral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x_train_data, n_x , n_y , n_z , keep_rate=0.7, seed=None ):\n",
    "\n",
    "    #Definition of the convolution net\n",
    "    \n",
    "    # conv => n_x*n_y*n_z\n",
    "    conv1 = tf.layers.conv3d(inputs=x_train_data, filters=32, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "    # conv => n_x*n_y*n_z\n",
    "    conv2 = tf.layers.conv3d(inputs=conv1, filters=32, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "    # pool => n_x*n_y*n_z/2*2*2\n",
    "    pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2)\n",
    "\n",
    "    # conv => n_x*n_y*n_z/2*2*2\n",
    "    conv4 = tf.layers.conv3d(inputs=pool3, filters=64, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "    # conv => n_x*n_y*n_z/2*2*2\n",
    "    conv5 = tf.layers.conv3d(inputs=conv4, filters=64, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "    # pool => n_x*n_y*n_z/4*4*4\n",
    "    pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2)\n",
    "\n",
    "    # conv => n_x*n_y*n_z/4*4*4\n",
    "    conv7 = tf.layers.conv3d(inputs=pool6, filters=128, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "    # conv => n_x*n_y*n_z/4*4*4\n",
    "    conv8 = tf.layers.conv3d(inputs=conv7, filters=128, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "    # pool => n_x*n_y*n_z/8*8*8\n",
    "    pool9 = tf.layers.max_pooling3d(inputs=conv8, pool_size=[2, 2, 2], strides=2)\n",
    "\n",
    "    cnn3d_bn = tf.layers.batch_normalization(inputs=pool9, training=True)\n",
    "\n",
    "    flattening = tf.reshape(cnn3d_bn, [-1, ((n_x//2)//2)//2*((n_y//2)//2)//2*((n_z//2)//2)//2*128])\n",
    "    dense = tf.layers.dense(inputs=flattening, units=1024, activation=tf.nn.relu)\n",
    "    # (1-keep_rate) is the probability that the node will be kept\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=True)\n",
    "\n",
    "    y_conv = tf.layers.dense(inputs=dropout, units=3)\n",
    "\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x_train_data, y_train_data, x_test_data, y_test_data, learning_rate=0.05, keep_rate=0.7, epochs=20, batch_size=128):\n",
    "\n",
    "    #dimensions of our input and output\n",
    "    n_x = 16\n",
    "    n_y = 16\n",
    "    n_z = 16\n",
    "    n_classes = 3\n",
    "    \n",
    "    # name of the .txt file that is created\n",
    "    name='First try'\n",
    "\n",
    "    f = open('./'+name+'.txt', 'w')\n",
    "    f.write('First Try ')\n",
    "\n",
    "    \n",
    "    x_input = tf.placeholder(tf.float32, shape=[None, 16, 16, 16, 1], name='Input' )\n",
    "    y_input = tf.placeholder(tf.float32, shape=[None, n_classes], name='Output') \n",
    "\n",
    "\n",
    "    prediction = cnn_model(x_input, n_x , n_y , n_z , keep_rate, seed = 1 , name='Prediction' )\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input), name='Cost' )\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "           \n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "    iterations = int(len(x_train_data)/batch_size) + 1\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        import datetime\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        iterations = int(len(x_train_data)/batch_size) + 1\n",
    "        # run epochs\n",
    "        for epoch in range(epochs):\n",
    "            start_time_epoch = datetime.datetime.now()\n",
    "            print('Epoch', epoch, 'started', end='')\n",
    "            epoch_loss = 0\n",
    "            train_acc = 0\n",
    "            # mini batch\n",
    "            for itr in range(iterations):\n",
    "                mini_batch_x = x_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                mini_batch_y = y_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                _optimizer, _cost = sess.run([optimizer, cost], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "                epoch_loss += _cost\n",
    "                train_acc += sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "            #  using mini batch in case not enough memory\n",
    "            acc = 0\n",
    "            itrs = int(len(x_test_data)/batch_size) + 1\n",
    "            for itr in range(itrs):\n",
    "                mini_batch_x_test = x_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                mini_batch_y_test = y_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                acc += sess.run(accuracy, feed_dict={x_input: mini_batch_x_test, y_input: mini_batch_y_test})\n",
    "            end_time_epoch = datetime.datetime.now()\n",
    "            print(' Training Set Accuracy:' + str(train_acc/itrs) +' Testing Set Accuracy:' + str(acc/itrs) + ' Time elapse: ' + str(end_time_epoch - start_time_epoch))\n",
    "            f.write('\\n'+' Training Set Accuracy:' + str(train_acc/itrs) +' Testing Set Accuracy:' + str(acc/itrs) + ' Time elapse: ' + str(end_time_epoch - start_time_epoch)) \n",
    "        end_time = datetime.datetime.now()\n",
    "        print('Time elapse: ', str(end_time - start_time))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started Training Set Accuracy:0.75 Testing Set Accuracy:0.5000000074505806 Time elapse: 0:00:02.388812\n",
      "Epoch 1 started Training Set Accuracy:0.8333333432674408 Testing Set Accuracy:0.2500000074505806 Time elapse: 0:00:02.252788\n",
      "Epoch 2 started Training Set Accuracy:0.5 Testing Set Accuracy:0.0833333358168602 Time elapse: 0:00:02.310382\n",
      "Epoch 3 started Training Set Accuracy:0.3333333358168602 Testing Set Accuracy:0.25 Time elapse: 0:00:02.307658\n",
      "Epoch 4 started Training Set Accuracy:0.6666666716337204 Testing Set Accuracy:0.2500000074505806 Time elapse: 0:00:02.276190\n",
      "Epoch 5 started Training Set Accuracy:0.3333333432674408 Testing Set Accuracy:0.416666679084301 Time elapse: 0:00:02.265710\n",
      "Epoch 6 started Training Set Accuracy:0.7500000074505806 Testing Set Accuracy:0.3333333432674408 Time elapse: 0:00:02.262014\n",
      "Epoch 7 started Training Set Accuracy:0.7500000149011612 Testing Set Accuracy:0.2500000074505806 Time elapse: 0:00:02.281332\n",
      "Epoch 8 started Training Set Accuracy:0.8333333358168602 Testing Set Accuracy:0.7500000149011612 Time elapse: 0:00:02.246496\n",
      "Epoch 9 started Training Set Accuracy:0.8333333358168602 Testing Set Accuracy:0.4166666716337204 Time elapse: 0:00:02.311283\n",
      "Epoch 10 started Training Set Accuracy:0.7500000074505806 Testing Set Accuracy:0.4166666716337204 Time elapse: 0:00:02.310868\n",
      "Epoch 11 started Training Set Accuracy:0.8333333358168602 Testing Set Accuracy:0.2500000074505806 Time elapse: 0:00:02.256420\n",
      "Epoch 12 started Training Set Accuracy:0.8333333432674408 Testing Set Accuracy:0.0833333358168602 Time elapse: 0:00:02.278349\n",
      "Epoch 13 started Training Set Accuracy:0.9166666716337204 Testing Set Accuracy:0.1666666716337204 Time elapse: 0:00:02.255715\n",
      "Epoch 14 started Training Set Accuracy:1.0 Testing Set Accuracy:0.1666666716337204 Time elapse: 0:00:02.250792\n",
      "Epoch 15 started Training Set Accuracy:0.9166666716337204 Testing Set Accuracy:0.7500000074505806 Time elapse: 0:00:02.279788\n",
      "Epoch 16 started Training Set Accuracy:1.0 Testing Set Accuracy:0.3333333432674408 Time elapse: 0:00:02.296337\n",
      "Epoch 17 started Training Set Accuracy:0.9166666716337204 Testing Set Accuracy:0.666666679084301 Time elapse: 0:00:02.315636\n",
      "Epoch 18 started Training Set Accuracy:1.0 Testing Set Accuracy:0.3333333358168602 Time elapse: 0:00:02.263181\n",
      "Epoch 19 started Training Set Accuracy:0.9166666716337204 Testing Set Accuracy:0.4166666716337204 Time elapse: 0:00:02.275822\n",
      "Epoch 20 started Training Set Accuracy:1.0 Testing Set Accuracy:0.3333333432674408 Time elapse: 0:00:02.272097\n",
      "Epoch 21 started Training Set Accuracy:1.0 Testing Set Accuracy:0.5833333358168602 Time elapse: 0:00:02.257477\n",
      "Epoch 22 started Training Set Accuracy:1.0 Testing Set Accuracy:0.5833333432674408 Time elapse: 0:00:02.274286\n",
      "Epoch 23 started Training Set Accuracy:1.0 Testing Set Accuracy:0.5833333432674408 Time elapse: 0:00:02.269347\n",
      "Epoch 24 started Training Set Accuracy:1.0 Testing Set Accuracy:0.25 Time elapse: 0:00:02.306697\n",
      "Epoch 25 started Training Set Accuracy:1.0 Testing Set Accuracy:0.5000000074505806 Time elapse: 0:00:02.313217\n",
      "Epoch 26 started Training Set Accuracy:1.0 Testing Set Accuracy:0.3333333432674408 Time elapse: 0:00:02.258305\n",
      "Epoch 27 started Training Set Accuracy:1.0 Testing Set Accuracy:0.2500000074505806 Time elapse: 0:00:02.287144\n",
      "Epoch 28 started Training Set Accuracy:1.0 Testing Set Accuracy:0.3333333432674408 Time elapse: 0:00:02.278333\n",
      "Epoch 29 started Training Set Accuracy:1.0 Testing Set Accuracy:0.7500000149011612 Time elapse: 0:00:02.254194\n",
      "Epoch 30 started Training Set Accuracy:1.0 Testing Set Accuracy:0.3333333358168602 Time elapse: 0:00:02.294961\n",
      "Epoch 31 started Training Set Accuracy:1.0 Testing Set Accuracy:0.416666679084301 Time elapse: 0:00:02.311934\n",
      "Epoch 32 started Training Set Accuracy:1.0 Testing Set Accuracy:0.4166666716337204 Time elapse: 0:00:02.253904\n",
      "Epoch 33 started Training Set Accuracy:1.0 Testing Set Accuracy:0.5833333432674408 Time elapse: 0:00:02.262526\n",
      "Epoch 34 started Training Set Accuracy:1.0 Testing Set Accuracy:0.5000000074505806 Time elapse: 0:00:02.279094\n",
      "Epoch 35 started Training Set Accuracy:1.0 Testing Set Accuracy:0.6666666716337204 Time elapse: 0:00:02.264849\n",
      "Epoch 36 started Training Set Accuracy:1.0 Testing Set Accuracy:0.2500000074505806 Time elapse: 0:00:02.263619\n",
      "Epoch 37 started Training Set Accuracy:1.0 Testing Set Accuracy:0.666666679084301 Time elapse: 0:00:02.278608\n",
      "Epoch 38 started Training Set Accuracy:1.0 Testing Set Accuracy:0.2500000074505806 Time elapse: 0:00:02.336246\n",
      "Epoch 39 started Training Set Accuracy:1.0 Testing Set Accuracy:0.4166666716337204 Time elapse: 0:00:02.284904\n",
      "Time elapse:  0:01:31.302942\n"
     ]
    }
   ],
   "source": [
    "namex='./XValues1.npy'\n",
    "namey='./YValues1.npy'\n",
    "\n",
    "x_train ,y_train = ImportValues(namex,namey)\n",
    "\n",
    "namex='./XValues2.npy'\n",
    "namey='./YValues2.npy'\n",
    "\n",
    "x_test ,y_test = ImportValues(namex,namey)\n",
    "\n",
    "\n",
    "train_neural_network(x_train, y_train, x_test, y_test, epochs=40, batch_size=3, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
