{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature and Lable Generation\n",
    "#### Here we read the stripped brain images, normalize them, make them to have the same size, and generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import fnmatch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import nibabel\n",
    "from nilearn.image import resample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lables of the dataset\n",
    "#### Here we read from the dataset the lables as *'Normal', 'MCI', 'LMCI', 'AD'* and merge the two middle classes into one. Also, here we extract the age and gender of the patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct patients =  790\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "ADNI1_Lables_dir = '../../Data_Set/Data_Lables/'\n",
    "\n",
    "alldata = pd.read_csv(ADNI1_Lables_dir + \"ADNI1AND2.csv\")      \n",
    "# print(alldata.shape)\n",
    "alldata = alldata[alldata.DX_Group != 'MCI']\n",
    "alldata = alldata[alldata.DX_Group != 'LMCI']\n",
    "alldata = alldata[alldata.DX_Group != 'SMC']\n",
    "# print(alldata.shape)\n",
    "alldata.groupby('DX_Group').count()\n",
    "dict_imageStatus = dict(zip(alldata.Subject_ID, alldata.DX_Group))  # Dict: k: patients' IDs, v: Alzheimer status\n",
    "dict_Age = dict(zip(alldata.Subject_ID, alldata.Age))               # Dict: k: patients' IDs, v: age\n",
    "dict_Sex = dict(zip(alldata.Subject_ID, alldata.Sex))               # Dict: k: patients' IDs, v: sex\n",
    "\n",
    "\n",
    "\n",
    "age_max = max(dict_Age.items(), key=operator.itemgetter(1))[1]\n",
    "dict_Age = {k: v/age_max for k, v in dict_Age.items()}  # Normalizing ages by max age among patients\n",
    "\n",
    "sex_to_num = {'M':1, 'F':0}     # A mask for converting sex to numbers\n",
    "dict_Sex = {k: sex_to_num[dict_Sex[k]] for k, v in dict_Sex.items()}\n",
    "\n",
    "num_patients = len(dict_imageStatus)\n",
    "print('number of distinct patients = ', num_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_scale(X_img , scale, nx_orig ,ny_orig,nz_orig  ):\n",
    "\n",
    "    \n",
    "    #X_img 3d image of size  nx_orig x ny_orig x nz_orig\n",
    "# Size of the original Image\n",
    "    nx_orig = 189\n",
    "    ny_orig = 212\n",
    "    nz_orig = 135 \n",
    "\n",
    "    \n",
    "#Calculate the new dimensions\n",
    "    nx = int(nx_orig//scale)\n",
    "    ny = int(ny_orig//scale)\n",
    "    nz = int(nz_orig//scale)\n",
    "    source_affine = np.eye(4) / scale\n",
    "\n",
    "#Interpolate in the new dimensions\n",
    "    img_nii_original = nibabel.Nifti1Image(X_img, affine=source_affine)\n",
    "    img_nii_scaled = resample_img(img_nii_original, target_affine=np.eye(4) , target_shape=(nx, ny, nz))\n",
    "    X_img_final = img_nii_scaled.get_data()\n",
    "    \n",
    "\n",
    "    return  X_img_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we read the size of the image with maximum size among the stripped MRI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of image with maximum dimensions among all images: [max_x, max_y, max_z]=  189 , 212 , 135\n",
      "Number of total images:  817\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# rootPath = '../Data_Set/test/' \n",
    "rootPath = '../../Data_Set/Data_Stripped_Images_Array/' \n",
    "\n",
    "file = open(rootPath + \"maxANDmin.txt\",\"r\") \n",
    "first_line = re.split(', | =',file.readline())\n",
    "file.close() \n",
    "\n",
    "max_x = int(first_line[3])\n",
    "max_y = int(first_line[4])\n",
    "max_z = int(first_line[5])\n",
    "\n",
    "print('The size of image with maximum dimensions among all images: [max_x, max_y, max_z]= ', max_x, \",\", max_y, \",\",  max_z)\n",
    "\n",
    "number_of_images = len([f for f in os.listdir(rootPath)]) - 2\n",
    "print('Number of total images: ', number_of_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of the dataset\n",
    "#### Here we process the images and convert them into X Y values in bataches of size 10 to be given as input to the 3DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  21 , No. of batches saved:  53\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  32 , No. of batches saved:  54\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  54 , No. of batches saved:  55\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  69 , No. of batches saved:  56\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  81 , No. of batches saved:  57\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  95 , No. of batches saved:  58\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  111 , No. of batches saved:  59\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  129 , No. of batches saved:  60\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  144 , No. of batches saved:  61\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  158 , No. of batches saved:  62\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  173 , No. of batches saved:  63\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  186 , No. of batches saved:  64\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  203 , No. of batches saved:  65\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  215 , No. of batches saved:  66\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  237 , No. of batches saved:  67\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  249 , No. of batches saved:  68\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  278 , No. of batches saved:  69\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  289 , No. of batches saved:  70\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  318 , No. of batches saved:  71\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  337 , No. of batches saved:  72\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  352 , No. of batches saved:  73\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  371 , No. of batches saved:  74\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  383 , No. of batches saved:  75\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  393 , No. of batches saved:  76\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  411 , No. of batches saved:  77\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  424 , No. of batches saved:  78\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  442 , No. of batches saved:  79\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  454 , No. of batches saved:  80\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  467 , No. of batches saved:  81\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  484 , No. of batches saved:  82\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  498 , No. of batches saved:  83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  520 , No. of batches saved:  84\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  540 , No. of batches saved:  85\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  552 , No. of batches saved:  86\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  563 , No. of batches saved:  87\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  577 , No. of batches saved:  88\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  596 , No. of batches saved:  89\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  614 , No. of batches saved:  90\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  626 , No. of batches saved:  91\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  646 , No. of batches saved:  92\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  658 , No. of batches saved:  93\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  674 , No. of batches saved:  94\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  684 , No. of batches saved:  95\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  700 , No. of batches saved:  96\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  713 , No. of batches saved:  97\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  725 , No. of batches saved:  98\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  741 , No. of batches saved:  99\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  756 , No. of batches saved:  100\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  769 , No. of batches saved:  101\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  787 , No. of batches saved:  102\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  797 , No. of batches saved:  103\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "116 130 83\n",
      "No. of images processed:  815 , No. of batches saved:  104\n",
      "116 130 83\n"
     ]
    }
   ],
   "source": [
    "[processedImgNo, counter] = [1, 1]    \n",
    "batchSize = 8\n",
    "batchNo   = 53\n",
    "status_to_num = {'Normal':0, 'AD':1}     # A mask for converting statuses to numbers \n",
    "\n",
    "# X_img: matrix of features \n",
    "# (number of samples (m) by number of features (n = max_x*max_y*max_z ))\n",
    "# Y: matrix of lables \n",
    "# (number of samples (m) by 1)\n",
    "X_img = np.array([])\n",
    "X_age = np.array([])\n",
    "X_sex = np.array([])\n",
    "Y     = np.array([])\n",
    "dict_imgData = dict()  # a dicionary from subject_ID to MRI picture, a 3rd order tensor\n",
    "\n",
    "pattern = '*.npy'\n",
    "\n",
    "# XY_Values_Path = '../XY_Values_BatchSize'+ str(batchSize) +'_Scaled_TwoGroups/'            # XY_Values: folder for saving X matrix and Y matrix\n",
    "XY_Values_Path = '../XY_Values_BatchSize'+ str(batchSize) +'_Scaled_TwoGroups_Flipped/'            # XY_Values: folder for saving X matrix and Y matrix\n",
    "if not os.path.exists(XY_Values_Path):\n",
    "    os.makedirs(XY_Values_Path)\n",
    "    \n",
    "    \n",
    "for root, dirs, files in os.walk(rootPath):\n",
    "    \n",
    "    for filename in fnmatch.filter(files, pattern):\n",
    "        \n",
    "        dataFiles = (os.path.join(root, filename))\n",
    "        key = str(filename[0:-4])              # subject_ID\n",
    "        val = np.load(dataFiles)\n",
    "\n",
    "        val_norm = val / np.max(val)\n",
    "\n",
    "        # padding val matrix with zeros to enlarge them upto the size max_x by max_y by max_z\n",
    "        [xshape, yshape, zshape] = [val_norm.shape[0], val_norm.shape[1], val_norm.shape[2]]\n",
    "        [xpad, ypad, zpad] = [int((max_x-xshape)*0.5), int((max_y-yshape)*0.5), int((max_z-zshape)*0.5)]\n",
    "        valPadded = np.array(np.pad(val_norm, ((xpad, xpad), (ypad, ypad), (zpad, zpad)), 'constant'))\n",
    "                    \n",
    "        if valPadded.shape[0]-max_x != 0:\n",
    "           nxpad = ((0, 1), (0, 0), (0, 0))\n",
    "           valPadded = np.pad(valPadded, pad_width=nxpad, mode='constant', constant_values=0)\n",
    "            \n",
    "        \n",
    "        if valPadded.shape[1]-max_y != 0 :\n",
    "           nypad = ((0, 0), (0, 1), (0, 0))\n",
    "           valPadded = np.pad(valPadded, pad_width=nypad, mode='constant', constant_values=0)\n",
    "\n",
    "        if valPadded.shape[2]-max_z != 0:\n",
    "           nzpad = ((0, 0), (0, 0), (0, 1))\n",
    "           valPadded = np.pad(valPadded, pad_width=nzpad, mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "            \n",
    "        assert  (valPadded.shape[0] == max_x) , \"x dimention != max_x\"\n",
    "        assert  (valPadded.shape[1] == max_y) , \"y dimention != max_y\"\n",
    "        assert  (valPadded.shape[2] == max_z) , \"z dimention != max_z\"\n",
    "        scale = 13.0/8.0\n",
    "        valPadded = load_and_scale(valPadded , scale, max_x, max_y, max_z  )\n",
    "        valPadded = np.flip(valPadded,2)\n",
    "        \n",
    "        if key in dict_imageStatus.keys():\n",
    "            print(valPadded.shape[0],valPadded.shape[1],valPadded.shape[2])\n",
    "\n",
    "        value = np.reshape(valPadded, [-1])                   # flattening the images data into a row vector\n",
    "            \n",
    "        if key in dict_imageStatus.keys():\n",
    "            \n",
    "            if counter % batchSize != 0:\n",
    "                \n",
    "                if X_img.size != 0:\n",
    "                        X_img = np.vstack([X_img, (value)])\n",
    "                        X_age = np.vstack([X_age, dict_Age[key]])\n",
    "                        X_sex = np.vstack([X_sex, dict_Sex[key]])\n",
    "                        Y = np.vstack([Y, status_to_num[dict_imageStatus[key]]])\n",
    "                        \n",
    "                        if processedImgNo == number_of_images:\n",
    "                            np.save(XY_Values_Path + 'X_Img_Values' + str(batchNo) + '.npy', X_img)\n",
    "                            np.save(XY_Values_Path + 'X_Age_Values' + str(batchNo) + '.npy', X_age)\n",
    "                            np.save(XY_Values_Path + 'X_Sex_Values' + str(batchNo) + '.npy', X_sex)\n",
    "                            np.save(XY_Values_Path + 'YValues' + str(batchNo) + '.npy', Y)\n",
    "\n",
    "                            print('No. of images processed: ', processedImgNo , ', No. of batches saved: ', batchNo)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                        X_img = (value)\n",
    "                        X_age = dict_Age[key]\n",
    "                        X_sex = dict_Sex[key]\n",
    "                        Y = status_to_num[dict_imageStatus[key]] \n",
    "                        \n",
    "            \n",
    "\n",
    "            if counter % batchSize == 0:\n",
    "                counter = 1\n",
    "#                 X_img = (value)\n",
    "#                 X_age = dict_Age[key]\n",
    "#                 X_sex = dict_Sex[key]\n",
    "#                 Y = status_to_num[dict_imageStatus[key]] \n",
    "                X_img = np.vstack([X_img, (value)])\n",
    "                X_age = np.vstack([X_age, dict_Age[key]])\n",
    "                X_sex = np.vstack([X_sex, dict_Sex[key]])\n",
    "                Y = np.vstack([Y, status_to_num[dict_imageStatus[key]]])\n",
    "                \n",
    "                \n",
    "                np.save(XY_Values_Path + 'X_Img_Values' + str(batchNo) + '.npy', X_img)\n",
    "                np.save(XY_Values_Path + 'X_Age_Values' + str(batchNo) + '.npy', X_age)\n",
    "                np.save(XY_Values_Path + 'X_Sex_Values' + str(batchNo) + '.npy', X_sex)\n",
    "                np.save(XY_Values_Path + 'YValues' + str(batchNo) + '.npy', Y)\n",
    "\n",
    "                print('No. of images processed: ', processedImgNo , ', No. of batches saved: ', batchNo)\n",
    "\n",
    "                batchNo = batchNo + 1\n",
    "                X_img = np.array([]) \n",
    "                X_sex = np.array([]) \n",
    "                X_age = np.array([]) \n",
    "                Y = np.array([]) \n",
    "                \n",
    "            else: \n",
    "                \n",
    "                counter = counter + 1\n",
    "        \n",
    "        processedImgNo = processedImgNo + 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
