{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature and Lable Generation\n",
    "#### Here we read the stripped brain images, normalize them, make them to have the same size, and generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "import matplotlib as plt\n",
    "import sys\n",
    "import os\n",
    "import fnmatch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lables of the dataset\n",
    "#### Here we read from the dataset the lables as *'Normal', 'MCI', 'LMCI', 'AD'* and merge the two middle classes into one. Also, here we extract the age and gender of the patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct patients =  1797\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "ADNI1_Lables_dir = '../DataSet/Data_Lables/'\n",
    "\n",
    "alldata = pd.read_csv(ADNI1_Lables_dir + \"ADNI1AND2.csv\")               \n",
    "dict_imageStatus = dict(zip(alldata.Subject_ID, alldata.DX_Group))  # Dict: k: patients' IDs, v: Alzheimer status\n",
    "dict_Age = dict(zip(alldata.Subject_ID, alldata.Age))               # Dict: k: patients' IDs, v: age\n",
    "dict_Sex = dict(zip(alldata.Subject_ID, alldata.Sex))               # Dict: k: patients' IDs, v: sex\n",
    "\n",
    "age_max = max(dict_Age.items(), key=operator.itemgetter(1))[1]\n",
    "dict_Age = {k: v/age_max for k, v in dict_Age.items()}  # Normalizing ages by max age among patients\n",
    "\n",
    "sex_to_num = {'M':1, 'F':0}     # A mask for converting sex to numbers\n",
    "dict_Sex = {k: sex_to_num[dict_Sex[k]] for k, v in dict_Sex.items()}\n",
    "\n",
    "num_patients = len(dict_imageStatus)\n",
    "print('number of distinct patients = ', num_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we read the size of the image with maximum size among the stripped MRI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of image with maximum dimensions among all images: [max_x, max_y, max_z]=  189 , 212 , 135\n",
      "Number of total images:  817\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "rootPath = '../DataSet/Data_Stripped_Images_Array/' \n",
    "\n",
    "file = open(rootPath + \"maxANDmin.txt\",\"r\") \n",
    "first_line = re.split(', | =',file.readline())\n",
    "file.close() \n",
    "\n",
    "max_x = int(first_line[3])\n",
    "max_y = int(first_line[4])\n",
    "max_z = int(first_line[5])\n",
    "\n",
    "print('The size of image with maximum dimensions among all images: [max_x, max_y, max_z]= ', max_x, \",\", max_y, \",\",  max_z)\n",
    "\n",
    "number_of_images = len([f for f in os.listdir(rootPath)]) - 2\n",
    "print('Number of total images: ', number_of_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of the dataset\n",
    "#### Here we process the images and convert them into X Y values in bataches of size 10 to be given as input to the 3DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of images processed:  10 , No. of batches saved:  1\n",
      "No. of images processed:  20 , No. of batches saved:  2\n",
      "No. of images processed:  30 , No. of batches saved:  3\n",
      "No. of images processed:  40 , No. of batches saved:  4\n",
      "No. of images processed:  50 , No. of batches saved:  5\n",
      "No. of images processed:  60 , No. of batches saved:  6\n",
      "No. of images processed:  70 , No. of batches saved:  7\n",
      "No. of images processed:  80 , No. of batches saved:  8\n",
      "No. of images processed:  90 , No. of batches saved:  9\n",
      "No. of images processed:  100 , No. of batches saved:  10\n",
      "No. of images processed:  110 , No. of batches saved:  11\n",
      "No. of images processed:  120 , No. of batches saved:  12\n",
      "No. of images processed:  130 , No. of batches saved:  13\n",
      "No. of images processed:  140 , No. of batches saved:  14\n",
      "No. of images processed:  150 , No. of batches saved:  15\n",
      "No. of images processed:  160 , No. of batches saved:  16\n",
      "No. of images processed:  170 , No. of batches saved:  17\n",
      "No. of images processed:  180 , No. of batches saved:  18\n",
      "No. of images processed:  190 , No. of batches saved:  19\n",
      "No. of images processed:  200 , No. of batches saved:  20\n",
      "No. of images processed:  210 , No. of batches saved:  21\n",
      "No. of images processed:  220 , No. of batches saved:  22\n",
      "No. of images processed:  230 , No. of batches saved:  23\n",
      "No. of images processed:  240 , No. of batches saved:  24\n",
      "No. of images processed:  250 , No. of batches saved:  25\n",
      "No. of images processed:  260 , No. of batches saved:  26\n",
      "No. of images processed:  270 , No. of batches saved:  27\n",
      "No. of images processed:  280 , No. of batches saved:  28\n",
      "No. of images processed:  290 , No. of batches saved:  29\n",
      "No. of images processed:  300 , No. of batches saved:  30\n",
      "No. of images processed:  310 , No. of batches saved:  31\n",
      "No. of images processed:  320 , No. of batches saved:  32\n",
      "No. of images processed:  330 , No. of batches saved:  33\n",
      "No. of images processed:  340 , No. of batches saved:  34\n",
      "No. of images processed:  350 , No. of batches saved:  35\n",
      "No. of images processed:  360 , No. of batches saved:  36\n",
      "No. of images processed:  370 , No. of batches saved:  37\n",
      "No. of images processed:  380 , No. of batches saved:  38\n",
      "No. of images processed:  390 , No. of batches saved:  39\n",
      "No. of images processed:  400 , No. of batches saved:  40\n",
      "No. of images processed:  410 , No. of batches saved:  41\n",
      "No. of images processed:  420 , No. of batches saved:  42\n",
      "No. of images processed:  430 , No. of batches saved:  43\n",
      "No. of images processed:  440 , No. of batches saved:  44\n",
      "No. of images processed:  450 , No. of batches saved:  45\n",
      "No. of images processed:  460 , No. of batches saved:  46\n",
      "No. of images processed:  470 , No. of batches saved:  47\n",
      "No. of images processed:  480 , No. of batches saved:  48\n",
      "No. of images processed:  490 , No. of batches saved:  49\n",
      "No. of images processed:  500 , No. of batches saved:  50\n",
      "No. of images processed:  510 , No. of batches saved:  51\n",
      "No. of images processed:  520 , No. of batches saved:  52\n",
      "No. of images processed:  530 , No. of batches saved:  53\n",
      "No. of images processed:  540 , No. of batches saved:  54\n",
      "No. of images processed:  550 , No. of batches saved:  55\n",
      "No. of images processed:  560 , No. of batches saved:  56\n",
      "No. of images processed:  570 , No. of batches saved:  57\n",
      "No. of images processed:  580 , No. of batches saved:  58\n",
      "No. of images processed:  590 , No. of batches saved:  59\n",
      "No. of images processed:  600 , No. of batches saved:  60\n",
      "No. of images processed:  610 , No. of batches saved:  61\n",
      "No. of images processed:  620 , No. of batches saved:  62\n",
      "No. of images processed:  630 , No. of batches saved:  63\n",
      "No. of images processed:  640 , No. of batches saved:  64\n",
      "No. of images processed:  650 , No. of batches saved:  65\n",
      "No. of images processed:  660 , No. of batches saved:  66\n",
      "No. of images processed:  670 , No. of batches saved:  67\n",
      "No. of images processed:  680 , No. of batches saved:  68\n",
      "No. of images processed:  690 , No. of batches saved:  69\n",
      "No. of images processed:  700 , No. of batches saved:  70\n",
      "No. of images processed:  710 , No. of batches saved:  71\n",
      "No. of images processed:  720 , No. of batches saved:  72\n",
      "No. of images processed:  730 , No. of batches saved:  73\n",
      "No. of images processed:  740 , No. of batches saved:  74\n",
      "No. of images processed:  750 , No. of batches saved:  75\n",
      "No. of images processed:  760 , No. of batches saved:  76\n",
      "No. of images processed:  770 , No. of batches saved:  77\n",
      "No. of images processed:  780 , No. of batches saved:  78\n",
      "No. of images processed:  790 , No. of batches saved:  79\n",
      "No. of images processed:  800 , No. of batches saved:  80\n",
      "No. of images processed:  810 , No. of batches saved:  81\n",
      "No. of images processed:  817 , No. of batches saved:  82\n"
     ]
    }
   ],
   "source": [
    "[processedImgNo, counter] = [1, 1]    \n",
    "[batchNo, batchSize] = [1, 10]\n",
    "status_to_num = {'Normal':0, 'SMC':1, 'MCI':1, 'LMCI':1, 'AD':2}     # A mask for converting statuses to numbers \n",
    "\n",
    "# X_img: matrix of features \n",
    "# (number of samples (m) by number of features (n = max_x*max_y*max_z ))\n",
    "# Y: matrix of lables \n",
    "# (number of samples (m) by 1)\n",
    "X_img = X_age = X_sex = Y = np.array([])\n",
    "\n",
    "\n",
    "dict_imgData = dict()  # a dicionary from subject_ID to MRI picture, a 3rd order tensor\n",
    "\n",
    "pattern = '*.npy'\n",
    "\n",
    "XY_Values_Path = '../DataSet/XY_Values/'            # XY_Values: folder for saving X matrix and Y matrix\n",
    "if not os.path.exists(XY_Values_Path):\n",
    "    os.makedirs(XY_Values_Path)\n",
    "    \n",
    "    \n",
    "for root, dirs, files in os.walk(rootPath):\n",
    "    \n",
    "    for filename in fnmatch.filter(files, pattern):\n",
    "        \n",
    "        dataFiles = (os.path.join(root, filename))\n",
    "        key = str(filename[0:-4])              # subject_ID\n",
    "        val = np.load(dataFiles) \n",
    "        val_norm = val / np.max(val)\n",
    "        \n",
    "        # padding val matrix with zeros to enlarge them upto the size max_x by max_y by max_z\n",
    "        [xshape, yshape, zshape] = [val_norm.shape[0], val_norm.shape[1], val_norm.shape[2]]\n",
    "        [xpad, ypad, zpad] = [int((max_x-xshape)*0.5), int((max_y-yshape)*0.5), int((max_z-zshape)*0.5)]\n",
    "        valPadded = np.array(np.pad(val_norm, ((xpad, xpad), (ypad, ypad), (zpad, zpad)), 'constant'))\n",
    "        \n",
    "        if valPadded.shape[0]-max_x != 0 or valPadded.shape[1]-max_y != 0 or valPadded.shape[2]-max_z != 0:\n",
    "            valPadded.resize(max_x, max_y, max_z)\n",
    "            \n",
    "        assert  (valPadded.shape[0] == max_x) , \"x dimention != max_x\"\n",
    "        assert  (valPadded.shape[1] == max_y) , \"y dimention != max_y\"\n",
    "        assert  (valPadded.shape[2] == max_z) , \"z dimention != max_z\"\n",
    "        \n",
    "        value = np.reshape(valPadded, [-1])                   # flattening the images data into a row vector\n",
    "            \n",
    "        if key in dict_imageStatus.keys():\n",
    "            \n",
    "            if counter % batchSize != 0:\n",
    "                \n",
    "                if X_img.size != 0:\n",
    "                        X_img = np.vstack([X_img, (value)])\n",
    "                        X_age = np.vstack([X_age, dict_Age[key]])\n",
    "                        X_sex = np.vstack([X_sex, dict_Sex[key]])\n",
    "\n",
    "                        Y = np.vstack([Y, status_to_num[dict_imageStatus[key]]])\n",
    "                        if processedImgNo == number_of_images:\n",
    "                            np.save(XY_Values_Path + 'X_Img_Values' + str(batchNo) + '.npy', X_img)\n",
    "                            np.save(XY_Values_Path + 'X_Age_Values' + str(batchNo) + '.npy', X_age)\n",
    "                            np.save(XY_Values_Path + 'X_Sex_Values' + str(batchNo) + '.npy', X_sex)\n",
    "\n",
    "                            np.save(XY_Values_Path + 'YValues' + str(batchNo) + '.npy', Y)\n",
    "\n",
    "                            print('No. of images processed: ', processedImgNo , ', No. of batches saved: ', batchNo)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                        X_img = (value)\n",
    "                        X_age = dict_Age[key]\n",
    "                        X_sex = dict_Sex[key]\n",
    "\n",
    "                        Y = status_to_num[dict_imageStatus[key]] \n",
    "\n",
    "\n",
    "            if counter % batchSize == 0:\n",
    "                \n",
    "                counter = 1\n",
    "                assert  (X_img.shape[0] == Y.shape[0])\n",
    "                assert  (X_img.shape[1] == max_x * max_y * max_z )  \n",
    "\n",
    "                np.save(XY_Values_Path + 'X_Img_Values' + str(batchNo) + '.npy', X_img)\n",
    "                np.save(XY_Values_Path + 'X_Age_Values' + str(batchNo) + '.npy', X_age)\n",
    "                np.save(XY_Values_Path + 'X_Sex_Values' + str(batchNo) + '.npy', X_sex)\n",
    "\n",
    "                np.save(XY_Values_Path + 'YValues' + str(batchNo) + '.npy', Y)\n",
    "\n",
    "                print('No. of images processed: ', processedImgNo , ', No. of batches saved: ', batchNo)\n",
    "\n",
    "                batchNo = batchNo + 1\n",
    "                X_img = np.array([]) \n",
    "                X_sex = np.array([]) \n",
    "                X_age = np.array([]) \n",
    "\n",
    "                Y = np.array([]) \n",
    "\n",
    "            else: \n",
    "                \n",
    "                counter = counter + 1\n",
    "            processedImgNo = processedImgNo + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
