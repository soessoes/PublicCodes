{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Soheil Esmaeilzadeh\n",
    "\n",
    "__Email:__ soheil.esmaeilzadeh@gmail.com / soes@stanford.edu\n",
    "\n",
    "__Date:__ 12/10/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecast by Deep Gated Recurrent Neural Network (DGRU)\n",
    "In this notebook we use __Deep Gated Recurrent Neural Network (DGRU)__ to __forecast__ a time series given its history information in the past. The model uses RNN and a hyperparameter study can easily be done by the user (e.g. changing the number of layers, number of epochs, size of each layer, drop-out coefficient, and etc.). At the end the forecasted results are compared against a __Decline Curve Analysis (DCA) approach__ for oil and gas applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x):\n",
    "    return datetime.strptime(x, '%Y%m')\n",
    "\n",
    "\n",
    "def differenced_series(dataset, interval=1):\n",
    "    '''\n",
    "    create a differenced time series\n",
    "    '''\n",
    "    dif = []\n",
    "    for i in range(interval, len(dataset)):\n",
    "        dif.append(dataset[i] - dataset[i - interval])\n",
    "    return Series(dif)\n",
    "\n",
    "\n",
    "def invert_differenced_series(history, yhat, interval=1):\n",
    "    '''\n",
    "    invert differenced value\n",
    "    '''\n",
    "    return yhat + history[-interval]\n",
    "\n",
    "\n",
    "def normalize(trainSet, testSet):\n",
    "    '''\n",
    "    scale train and test dataset to [-1, 1]\n",
    "    '''\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(trainSet)\n",
    "\n",
    "    trainSet = trainSet.reshape(trainSet.shape[0], trainSet.shape[1])\n",
    "    trainSet_scaled = scaler.transform(trainSet)\n",
    "\n",
    "    testSet = testSet.reshape(testSet.shape[0], testSet.shape[1])\n",
    "    testSet_scaled = scaler.transform(testSet)\n",
    "    return scaler, trainSet_scaled, testSet_scaled\n",
    "\n",
    "def invert_normalize(scaler, X, value):\n",
    "    '''\n",
    "    invert scaled train and test dataset back\n",
    "    '''\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    return scaler.inverse_transform(array.reshape(1, len(array)))[0, -1]\n",
    "\n",
    "def RMSPE(x,y):\n",
    "    result=0\n",
    "    for i in range(len(x)):\n",
    "        result += ((y[i]-x[i])/x[i])**2\n",
    "    result /= len(x)\n",
    "    result = sqrt(result)\n",
    "    result *= 100\n",
    "    return result\n",
    "\n",
    "\n",
    "def fit_RNN(train, batch_size, nb_epoch, neurons, dropout):\n",
    "    '''\n",
    "    # fit an RNN network to training data\n",
    "    '''\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    X = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "    model = Sequential()\n",
    "    for i in range(len(neurons)-1):\n",
    "        model.add(GRU(neurons[i], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), \n",
    "                            kernel_regularizer=regularizers.l2(0.01), \n",
    "                            recurrent_regularizer=regularizers.l2(0.01), \n",
    "                            bias_regularizer=regularizers.l2(0.01), \n",
    "                            activation='tanh', \n",
    "                            stateful=True,return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(GRU(neurons[len(neurons)-1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), \n",
    "                        kernel_regularizer=regularizers.l2(0.01), \n",
    "                        recurrent_regularizer=regularizers.l2(0.01), \n",
    "                        bias_regularizer=regularizers.l2(0.01),\n",
    "                        activation='tanh', \n",
    "                        stateful=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    for i in range(nb_epoch):\n",
    "        print('epoch:',i+1)\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def forecast_RNN(model, batch_size, X):\n",
    "    '''\n",
    "    # make a one-step forecast\n",
    "    '''\n",
    "    X = X.reshape(1, len(X), 1)\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,0]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    '''\n",
    "    convert an values into a dataset  x, y values\n",
    "    '''\n",
    "    dataset = np.insert(dataset,[0]*look_back,0)    \n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    dataY= np.array(dataY)        \n",
    "    dataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "    dataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(look_back,neurons,n_epoch, dropout):\n",
    "\n",
    "    series = read_csv('TimeSeries.csv', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
    "    series.head()\n",
    "\n",
    "    raw_values = series.values\n",
    "    # transform data to be stationary\n",
    "    diff = differenced_series(raw_values, 1)\n",
    "\n",
    "\n",
    "    # create dataset x,y\n",
    "    dataset = diff.values # raw_values[:-1] \n",
    "    dataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(dataset.shape[0] * 0.75)\n",
    "    test_size = dataset.shape[0] - train_size\n",
    "    train, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = normalize(train, test)\n",
    "\n",
    "\n",
    "    # fit the model\n",
    "    RNN_model = fit_RNN(train_scaled, 1, n_epoch, neurons, dropout)\n",
    "    # forecast the entire training dataset to build up state for forecasting\n",
    "    print('Forecasting Training Data')   \n",
    "    predictions_train = list()\n",
    "    for i in range(len(train_scaled)):\n",
    "        # make one-step forecast\n",
    "        X, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "        yhat = forecast_RNN(RNN_model, 1, X)\n",
    "        # invert scaling\n",
    "        yhat = invert_normalize(scaler, X, yhat)\n",
    "        # invert differencing\n",
    "        yhat = invert_differenced_series(raw_values, yhat, len(raw_values)-i)\n",
    "        # store forecast\n",
    "        predictions_train.append(yhat)\n",
    "        expected = raw_values[ i+1 ] \n",
    "    print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "    # report performance\n",
    "#     train_rmse = sqrt(mean_squared_error(raw_values[1:len(train_scaled)+1], predictions_train))/np.mean(raw_values[1:len(train_scaled)+1])*100\n",
    "    train_rmspe = RMSPE(raw_values[1:len(train_scaled)+1],predictions_train)\n",
    "    \n",
    "    # forecast the test data\n",
    "    print('Forecasting Testing Data')\n",
    "    predictions_test = list()\n",
    "    for i in range(len(test_scaled)):\n",
    "        # make one-step forecast\n",
    "        X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "        yhat = forecast_RNN(RNN_model, 1, X)\n",
    "        # invert scaling\n",
    "        yhat = invert_normalize(scaler, X, yhat)\n",
    "        # invert differencing\n",
    "        yhat = invert_differenced_series(raw_values, yhat, len(test_scaled)+1-i)\n",
    "        # store forecast\n",
    "        predictions_test.append(yhat)\n",
    "        expected = raw_values[len(train) + i + 1]\n",
    "    print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "    # report performance using RMSE\n",
    "#     test_rmse = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))/np.mean(raw_values[-len(test_scaled):])*100\n",
    "\n",
    "    test_rmspe = RMSPE(raw_values[-len(test_scaled):],predictions_test)\n",
    "    \n",
    "    predictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\n",
    "    return predictions, train_rmspe, test_rmspe\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter set number: 1\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 20s 121ms/step - loss: 0.6850\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 18s 107ms/step - loss: 0.2036\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 16s 96ms/step - loss: 0.1365\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 16s 97ms/step - loss: 0.1285\n",
      "epoch: 5\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 17s 101ms/step - loss: 0.1277\n",
      "epoch: 6\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 87ms/step - loss: 0.1276\n",
      "epoch: 7\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 90ms/step - loss: 0.1274\n",
      "epoch: 8\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 19s 113ms/step - loss: 0.1274\n",
      "epoch: 9\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 90ms/step - loss: 0.1274\n",
      "epoch: 10\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 14s 83ms/step - loss: 0.1271\n",
      "epoch: 11\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 14s 83ms/step - loss: 0.1274\n",
      "epoch: 12\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 14s 84ms/step - loss: 0.1275\n",
      "epoch: 13\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 18s 108ms/step - loss: 0.1276\n",
      "epoch: 14\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 17s 101ms/step - loss: 0.1274\n",
      "epoch: 15\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 90ms/step - loss: 0.1276\n",
      "Forecasting Training Data\n",
      "Month=169, Predicted=6.697693, Expected=6.600100\n",
      "Forecasting Testing Data\n",
      "Month=57, Predicted=7.434693, Expected=7.440200\n",
      "Train RMSE: 6.6827\n",
      "Test RMSE: 10.4791\n",
      "Hyperparameter set number: 2\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 22s 130ms/step - loss: 0.6741\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 19s 112ms/step - loss: 0.2036\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 18s 106ms/step - loss: 0.1398\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 16s 96ms/step - loss: 0.1292\n",
      "epoch: 5\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 16s 94ms/step - loss: 0.1282\n",
      "epoch: 6\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 16s 95ms/step - loss: 0.1273\n",
      "epoch: 7\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 16s 96ms/step - loss: 0.1274\n",
      "epoch: 8\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 91ms/step - loss: 0.1267\n",
      "epoch: 9\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 16s 92ms/step - loss: 0.1266\n",
      "epoch: 10\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 92ms/step - loss: 0.1275\n",
      "epoch: 11\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 91ms/step - loss: 0.1268\n",
      "epoch: 12\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 18s 105ms/step - loss: 0.1277\n",
      "epoch: 13\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 19s 112ms/step - loss: 0.1272\n",
      "epoch: 14\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 19s 114ms/step - loss: 0.1279\n",
      "epoch: 15\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 15s 91ms/step - loss: 0.1278\n",
      "Forecasting Training Data\n",
      "Month=169, Predicted=6.690295, Expected=6.600100\n",
      "Forecasting Testing Data\n",
      "Month=57, Predicted=7.427295, Expected=7.440200\n",
      "Train RMSE: 6.6759\n",
      "Test RMSE: 10.4703\n",
      "Hyperparameter set number: 3\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 14s 82ms/step - loss: 0.3458\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 12s 70ms/step - loss: 0.1893\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 11s 63ms/step - loss: 0.1430\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 11s 63ms/step - loss: 0.1293\n",
      "epoch: 5\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 11s 62ms/step - loss: 0.1232\n",
      "epoch: 6\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.1202\n",
      "epoch: 7\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 10s 59ms/step - loss: 0.1203\n",
      "epoch: 8\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 11s 62ms/step - loss: 0.1213\n",
      "epoch: 9\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 14s 83ms/step - loss: 0.1192\n",
      "epoch: 10\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 12s 68ms/step - loss: 0.1187\n",
      "epoch: 11\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 10s 57ms/step - loss: 0.1192\n",
      "epoch: 12\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.1202\n",
      "epoch: 13\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 55ms/step - loss: 0.1180\n",
      "epoch: 14\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 10s 57ms/step - loss: 0.1191\n",
      "epoch: 15\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.1175\n",
      "Forecasting Training Data\n",
      "Month=169, Predicted=6.498726, Expected=6.600100\n",
      "Forecasting Testing Data\n",
      "Month=57, Predicted=7.341367, Expected=7.440200\n",
      "Train RMSE: 6.0502\n",
      "Test RMSE: 9.8267\n",
      "Hyperparameter set number: 4\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 12s 73ms/step - loss: 0.3510\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 55ms/step - loss: 0.1941\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 55ms/step - loss: 0.1483\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 55ms/step - loss: 0.1338\n",
      "epoch: 5\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.1285\n",
      "epoch: 6\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.1295\n",
      "epoch: 7\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.1261\n",
      "epoch: 8\n",
      "Epoch 1/1\n",
      "169/169 [==============================] - 14s 82ms/step - loss: 0.1252\n",
      "epoch: 9\n",
      "Epoch 1/1\n",
      "156/169 [==========================>...] - ETA: 0s - loss: 0.1249"
     ]
    }
   ],
   "source": [
    "# change for hyperparameter study\n",
    "look_back= [25]\n",
    "neurons = [[25,20,15], [15,5]]\n",
    "# n_epoch = [15,15,15]\n",
    "dropout = [0.1,0.25]\n",
    "hyperParameters = []\n",
    "pred = []\n",
    "train_err = []\n",
    "test_err = []\n",
    "count = 0\n",
    "for i in range(len(look_back)):\n",
    "    for ii in range(len(neurons)):\n",
    "        for j in range(len(dropout)):\n",
    "            count = count + 1\n",
    "            print('Hyperparameter set number:', count)\n",
    "            [predictions, train_rmspe, test_rmspe] = experiment(look_back[i], neurons[ii], 15, dropout[j])\n",
    "            pred.append(predictions)\n",
    "            train_err.append(train_rmspe)\n",
    "            test_err.append(test_rmspe)\n",
    "            hyperParameters.append([look_back[i], neurons[ii], dropout[j]])\n",
    "            print('Train RMSE: %.4f' % train_rmspe)\n",
    "            print('Test RMSE: %.4f' % test_rmspe)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hyper parameter study results\n",
    "with open('./Code3_Results/hyperParameters.py', 'w') as f:\n",
    "    f.write('score = %s' % hyperParameters)\n",
    "with open('./Code3_Results/train_rmspe.py', 'w') as f:\n",
    "    f.write('score = %s' % train_rmspe)\n",
    "with open('./Code3_Results/test_rmspe.py', 'w') as f:\n",
    "    f.write('score = %s' % test_rmspe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Train RMSE: %.4f' % train_rmse)\n",
    "# print('Test RMSE: %.4f' % test_rmse)\n",
    "\n",
    "series = read_csv('TimeSeries.csv', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
    "raw_values = series.values\n",
    "    \n",
    "plt.rcParams[\"axes.titlesize\"] = 50\n",
    "plt.rcParams[\"legend.fontsize\"] = 30\n",
    "plt.rcParams[\"axes.labelsize\"] = 36\n",
    "\n",
    "'''\n",
    "plot the actual data v.s. predicted data\n",
    "'''\n",
    "x = np.asanyarray(range(len(pred[3])))\n",
    "y = 11.5*np.exp(-x/240)\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(raw_values, label='Original Time Series', color='blue',linewidth=3.0,linestyle=':')\n",
    "ax.plot(pred[3], label='Time Series Prediction', color='red',linewidth=3.0)\n",
    "ax.plot(x[60:],y[60:],  label='DCA Prediction', color='orange',linewidth=8, linestyle='--')\n",
    "# ax.plot(DCA_Prediction, label='DCA Prediction', color='black')\n",
    "ax.axvline(x=len(raw_values)*0.75+1,color='k', linewidth=5, linestyle='--')\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_xlabel('Time')\n",
    "ax.xaxis.set_tick_params(labelsize=34)\n",
    "ax.yaxis.set_tick_params(labelsize=34)\n",
    "\n",
    "# plt.ylabel('oil production '+ r'$(10^4 m^3)$',fontsize = 16)\n",
    "ax.set_ylabel(r'Oil Production $\\times 10^3$ [BBL]')\n",
    "fig.savefig('./Code3_Results/LSTMResult.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
